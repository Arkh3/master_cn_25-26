{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d6398d",
   "metadata": {},
   "source": [
    "# Numerical Computing: Homework 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89be079",
   "metadata": {},
   "source": [
    "###### Authors:\n",
    "\n",
    "* alberto.suarez@uam.es\n",
    "* Student 1: Andrés Teruel Fernández (andres.teruel@estudiante.uam.es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50c407",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "* Turn in a single zip file\n",
    "    1. Make sure that all cells can be exectued sequentially and without errors.\n",
    "    2. The folder structure should allow the direct execution of the notebook without any modifications.\n",
    "    3. All functions should be defined in a separate file. The notebook should contain only code to illustrate the answers.\n",
    "    4. Submit a single file named: CN_2024_2025_HW2_\\<lastNameStudent1\\>_\\<lastNameStudent2\\>.zip\n",
    "\n",
    "Example:     CN_2024_2025_HW2_moran_puig.zip\n",
    "\n",
    "* The code should follow \n",
    "    * PEP 20 – The Zen of Python. https://peps.python.org/pep-0020    \n",
    "    * PEP 8 – Style Guide for Python Code. <https://peps.python.org/pep-0008>\n",
    "    * PEP 257 – Docstring Conventions. https://peps.python.org/pep-0257\n",
    "    * Type hints. https://docs.python.org/3/library/typing.html\n",
    "    * Mypy. https://mypy.readthedocs.io/en/stable/cheat_sheet_py3.html\n",
    "    * Google Python sytle guide. https://google.github.io/styleguide/pyguide.html  \n",
    "\n",
    "* Please use Markdown [https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet] cells with formulas in latex for the derivations. Alternatively, insert a scanned image of the derivations, as it is here done with the Alan Turing [https://www.turing.org.uk/] picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numerical_computing_HW_002_calculus as nc\n",
    "# import my_numerical_computing_HW_002_calculus as nc\n",
    "\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3278e",
   "metadata": {},
   "source": [
    "### Taylor expansion in 1 dimension.\n",
    "\n",
    "Consider a sufficiently smooth function $f(x)$, whose derivatives at $x_0$ exist up to order $K+1$, with the $(K+1)$th derivative continuous at that point. \n",
    "\n",
    "In the neighborhood of $x_0$, the function can be approximated by the (Taylor) series \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(x) & = & \\sum_{k=0}^{\\infty} \\frac{1}{k!} f^{(k)}\\left(x_0\\right)  \\left(x-x_0\\right)^k \n",
    "      = f(x_0) + f^{(1)}\\left(x_0\\right)  \\left(x-x_0\\right)\n",
    "     + \\frac{1}{2!} f^{(2)}\\left(x_0\\right)  \\left(x-x_0\\right)^2 \n",
    "     + \\frac{1}{3!} f^{(3)}\\left(x_0\\right)  \\left(x-x_0\\right)^3 \n",
    "     + \\ldots, \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f^{(0)}\\left(x_0\\right) & = & f(x_0), \\\\\n",
    "f^{(k)}\\left(x_0\\right) & = & \\left. \\frac{d^k f(x)}{d x^k}\\right|_{x=x_0}, \\ \\ k = 1, 2, \\ldots\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "The series converges for $\\left| x - x_0 \\right| < R$, where $R$ is the radius of convergence.\n",
    "The radius of convergence of the series is the distance from $x_0$ to the closest singularity of the function in the *complex* plane; that is, extending the definition of the function so that its argument is a complex number: $f(z)$, with $z\\in \\mathbb{C}$.\n",
    "\n",
    "In the neighborhood of $x_0$, the function can be approximated by the (Taylor) polynomial: \n",
    "\\begin{eqnarray}\n",
    "f(x) & = & \\sum_{k=0}^{K} \\frac{1}{k!} f^{(k)}\\left(x_0\\right)  \\left(x-x_0\\right)^k + E_K(x)\\\\\n",
    "     & = & f(x_0) + f^{(1)}\\left(x_0\\right)  \\left(x-x_0\\right) + \\frac{1}{2!} f^{(2)}\\left(x_0\\right)  \\left(x-x_0\\right)^2 + \\ldots + \\frac{1}{K!} f^{(K)}\\left(x_0\\right)  \\left(x-x_0\\right)^K  + E_K(x).\n",
    "\\end{eqnarray}\n",
    "The residual (error of the approximation) in the Lagrange form is \n",
    "$$\n",
    "E_K(x) = \\frac{1}{(K+1)!} f^{(K+1)}\\left(\\xi\\right) \\left(\\xi - x_0\\right)^{K+1}, \\quad \\text{ for some } \\xi \\in \\left[x_0, x \\right].\n",
    "$$\n",
    "\n",
    "Therefore, the approximation error due to truncation of the series is $\\mathcal{O}\\left( \\left|x - x_0\\right|^{K+1}\\right)$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f(x) & \\approx & f(x_0) + f^{(1)}\\left(x_0\\right)  \\left(x-x_0\\right) + \\frac{1}{2!} f^{(2)}\\left(x_0\\right)  \\left(x-x_0\\right)^2 + \\ldots + \\frac{1}{K!} f^{(K)}\\left(x_0\\right)  \\left(x-x_0\\right)^K  + \\mathcal{O}\\left( \\left(x - x_0\\right)^{K+1}\\right).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "*Example*: \n",
    "The Taylor series of the exponential function is\n",
    "$$\n",
    "e^{x} = \\sum_{k=0}^{\\infty} \\frac{1}{k!} x^k, \\quad \\left| \\right| < \\infty$.\n",
    "$$\n",
    "\n",
    "To see the radius of convergence of the series is $R = \\infty$, we consider \n",
    "the extension of the definition of the exponential to the complex plane, and observe, using Euler's formula that\n",
    "$$\n",
    "e^z = e^{\\text{Re}(z)} e^{i \\text{Im}(z)}  = e^{\\text{Re}(z)} \\left( \\cos\\left(\\text{Im}(z)\\right) + i \\sin\\left(\\text{Im}(z)\\right)  \\right),\n",
    "$$\n",
    "has no singularities for finite $z \\in \\mathbb{C}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8a19f",
   "metadata": {},
   "source": [
    "### Taylor expansion in several dimensions.\n",
    "$$\n",
    "f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + (\\mathbf{x} - \\mathbf{x}_0)^\\top \\boldsymbol{\\nabla} f(\\mathbf{x}_0) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{x}_0)^\\top \\mathbf{H}(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{3!} (\\mathbf{x} - \\mathbf{x}_0)^\\top \\mathbf{T}(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0) + \\mathcal{O}\\left( \\left\\| \\mathbf{x} - \\mathbf{x}_0 \\right\\|^4 \\right),\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\boldsymbol{\\nabla} f(\\mathbf{x}_0) = \\left. \\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}_0} \n",
    "$$ \n",
    "is the gradient of the function evaluated at $\\mathbf{x}_0$. \n",
    "The elements of the gradient are:\n",
    "$$\n",
    "\\nabla_i f(\\mathbf{x}_0) = \\left. \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\right|_{\\mathbf{x}=\\mathbf{x}_0}.\n",
    "$$\n",
    "Using index notation, the second order term in the Taylor series is\n",
    "$$\n",
    "\\sum_{i=1}^D  \\left(\n",
    "\\left. \\frac{\\partial f(\\mathbf{x})}{\\partial x_i}\\right|_{\\mathbf{x}=\\mathbf{x}_0}  (x_i - x_{0,i})\n",
    "\\right).\n",
    "$$\n",
    "The Hessian matrix is \n",
    "$$\n",
    "\\mathbf{H}(\\mathbf{x}_0) = \\boldsymbol{\\nabla} \\boldsymbol{\\nabla} f(\\mathbf{x}_0) = \\left. \\frac{\\partial^2 f(\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}_0}.\n",
    "$$\n",
    "The elements of the Hessian matrix are the second derivatives of the function, evaluated at $ \\mathbf{x}_0$:\n",
    "$$\n",
    "H_{ij}(\\mathbf{x}_0) = \\left. \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right|_{\\mathbf{x}=\\mathbf{x}_0}.\n",
    "$$\n",
    "Using index notation, the second order term in the Taylor series is\n",
    "$$\n",
    "\\frac{1}{2!} \\sum_{i=1}^D \\sum_{j=1}^D \\left( \n",
    "\\left. \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right|_{\\mathbf{x}=\\mathbf{x}_0}  (x_i - x_{0,i}) (x_j - x_{0,j})\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "The third-derivative tensor evaluate at $\\mathbf{x}_0$ is \n",
    "$$\n",
    "\\mathbf{T}(\\mathbf{x}_0) \n",
    "= \\boldsymbol{\\nabla}\\boldsymbol{\\nabla} \\boldsymbol{\\nabla} f(\\mathbf{x}_0) \n",
    "= \\left. \\frac{\\partial^3 f(\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x} \\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}_0}. \n",
    "$$\n",
    "Using index notation, the third order term in the Taylor series is\n",
    "$$\n",
    "\\frac{1}{3!} \\sum_{i=1}^D \\sum_{j=1}^D \\sum_{k=1}^D \\left(\n",
    "\\left. \\frac{\\partial^3 f(\\mathbf{x})}{\\partial x_i \\partial x_j \\partial x_k}\\right|_{\\mathbf{x}=\\mathbf{x}_0}  (x_i - x_{0,i}) (x_j - x_{0,j}) (x_k - x_{0,k}) \n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42687773",
   "metadata": {},
   "source": [
    "### Exercise 1: Taylor expansion\n",
    "\n",
    "Implement the function `taylor_approximation` in the file `my_numerical_computation_HW_002_calculus.py`, which has been imported as `nc`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d579a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Taylor approximations of different orders\n",
    "\n",
    "def plot_taylor_approximation(\n",
    "    function: Callable[[float], float], \n",
    "    function_derivative: Callable[[float, int], float], \n",
    "    K: int,\n",
    "    x_0: float,\n",
    "    interval_plot: Tuple[float, float],\n",
    "    figure_size: Tuple[int, int],\n",
    ") -> None:\n",
    "    \n",
    "    x = np.linspace(*interval_plot, num=1000)\n",
    "    y_exact = function(x)\n",
    "    y_taylor = nc.taylor_approximation(x, function, function_derivative, K, x_0)\n",
    "    \n",
    "    fig, axs = plt.subplots(K + 1, 2, sharex=True, figsize=figure_size)\n",
    "    \n",
    "    for k, y in enumerate(y_taylor):\n",
    "        axs[k, 0].plot(x, y_exact, label='exact')\n",
    "        axs[k, 0].plot(x, y_taylor[k, :], label='Taylor(k = {:d})'.format(k))\n",
    "        axs[k, 0].set_xlabel('$x$')\n",
    "        axs[k, 0].set_ylabel('$f(x)$')\n",
    "        axs[k, 0].legend()\n",
    "    \n",
    "        error = y_taylor[k, :] - y_exact \n",
    "        axs[k, 1].plot(x, error, label='error')\n",
    "        axs[k, 1].set_xlabel('$x$')\n",
    "        axs[k, 1].set_ylabel('$error$')\n",
    "        axs[k, 1].legend()\n",
    "        axs[k, 1].axhline(y = 0.0, color = 'k', linestyle = ':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db9913f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numerical_computing_HW_002_calculus' has no attribute 'taylor_approximation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m      4\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mplot_taylor_approximation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_derivative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigure_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m, in \u001b[0;36mplot_taylor_approximation\u001b[0;34m(function, function_derivative, K, x_0, interval_plot, figure_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m*\u001b[39minterval_plot, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     13\u001b[0m y_exact \u001b[38;5;241m=\u001b[39m function(x)\n\u001b[0;32m---> 14\u001b[0m y_taylor \u001b[38;5;241m=\u001b[39m \u001b[43mnc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtaylor_approximation\u001b[49m(x, function, function_derivative, K, x_0)\n\u001b[1;32m     16\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(K \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, figsize\u001b[38;5;241m=\u001b[39mfigure_size)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_taylor):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numerical_computing_HW_002_calculus' has no attribute 'taylor_approximation'"
     ]
    }
   ],
   "source": [
    "# Taylor approximation for the negative exponential \n",
    "    \n",
    "x_0 = 1.0\n",
    "K = 10\n",
    "plot_taylor_approximation(\n",
    "    function=lambda x: np.exp(- x), \n",
    "    function_derivative=lambda x, k: (1 - 2 * (k % 2)) * np.exp(- x), \n",
    "    K=K,\n",
    "    x_0=x_0,\n",
    "    interval_plot=(x_0 - 2.0, x_0 + 3.0),\n",
    "    figure_size=(K, 4*K),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353ba82",
   "metadata": {},
   "source": [
    "### Exercise 2: Properties of the Taylor expansion.\n",
    "\n",
    "For each of these functions\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f_1(x) & = &  \\sin(x), \\\\\n",
    "f_2(x) & = &  \\log{(1 + x)}, \\\\\n",
    "f_3(x) & = &  \\frac{1}{1 + x}, \\\\\n",
    "f_4(x) & = &  \\frac{1}{1 + x^2}.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "1. Provide an expression of the Taylor series around $x_0 = 0.0$ of the form\n",
    "$$\n",
    "f(x) = \\sum_{k=0}^{\\infty} c_k x^k.\n",
    "$$\n",
    "In other words, find the expression of $c_k$ in terms of $k$ for each of the functions considered.\n",
    "2. Determine the radius of covergence of each of these series by locating the singularity that is closest to $x_0$ in the complex plane. \n",
    "3. Make a plot similar to the one in the previous cell to illustrate the convergence of these approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3b0f7",
   "metadata": {},
   "source": [
    "### Exercise 3: Asymptotic expansions.\n",
    "\n",
    "In some cases is useful to make expansions around singular points. https://aofa.cs.princeton.edu/40asymptotic/\n",
    "\n",
    "Of particular interest are expansions about infinity.\n",
    "\n",
    "For instance,\n",
    "$$\n",
    "\\log \\left(N + 1 \\right) = \\log N + \\log\\left(1 + \\frac{1}{N}\\right) \\sim  \\log N + \\frac{1}{N} - \\frac{1}{2} \\frac{1}{N^2}  + \\mathcal{O}\\left(\\frac{1}{N^3}\\right) \n",
    "$$\n",
    "\n",
    "For the factorial of large numbers, one can use Stirling's approximation:\n",
    "$$\n",
    "N! = \\Gamma \\left(N + 1\\right) \\approx \\sqrt{2 \\pi N} \\left( \\frac{N}{e} \\right)^N \\left( 1 + \\frac{1}{12 N} + \\frac{1}{288 N^2} \\right) + \\mathcal{O}\\left(\\frac{1}{N^3} \\right)\n",
    "$$\n",
    "where $\\Gamma(x)$ with $x \\in \\mathbb{R}$ is the Gamma function [https://en.wikipedia.org/wiki/Gamma_function](https://en.wikipedia.org/wiki/Gamma_function)].\n",
    "\n",
    "\n",
    "* Explore the dependence on $N$ of the magnitude of the error of Stirling's approximation truncanted at different orders.  \n",
    "* Determine the smallest values of $N$ for which the approximation $N! = \\Gamma \\left(N + 1\\right) \\sqrt{2 \\pi N} \\left( \\frac{N}{e} \\right)^N$ has relative errors below $1 \\%$, $1$ per thousand, and $1.0e-5$.\n",
    "\n",
    "NOTE: \n",
    "If $N$ is very large, it may be necessary to use the logarithm of the factorial\n",
    "$$\n",
    "\\log N! \n",
    "= \\log \\prod_{n=1}^N n \n",
    "= \\sum_{n=1}^N \\log n \n",
    "\\approx N \\log N - N + \\frac{1}{2} \\log N + \\frac{1}{2} \\log(2 \\pi) + \\frac{1}{12 N} - \\frac{1}{360 N^3} \\mathcal{O}\\left(\\frac{1}{N^4} \\right)\n",
    "$$\n",
    "\n",
    "REMINDER: Notation for the asymptotic behavior of functions.\n",
    "\n",
    "Given a function $f(N)$, with $N \\rightarrow \\infty$ \n",
    "* $g(N) = \\mathcal{O}\\left(f(N)\\right)$ means: $\\left| \\frac{g(N)}{f(N)} \\right|$ as $N \\rightarrow \\infty$.\n",
    "* $g(N) = \\mathcal{o}\\left(f(N)\\right)$ means: $\\lim_{N \\rightarrow \\infty} \\frac{g(N)}{f(N)} = 0$. \n",
    "* $g(N) \\sim f(N)$  means: $\\lim_{N \\rightarrow \\infty} \\frac{g(N)}{f(N)} = 1$.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcdbdba",
   "metadata": {},
   "source": [
    "### Exercise 4. Estimation of derivatives by divided differences.\n",
    "\n",
    "The derivative of a function $f(x)$ at $x_0$ measures the rate of variation of the function at that point.\n",
    "\n",
    "We are given three formulas to calculate the derivative numerically:\n",
    "* Left derivative:\n",
    "$$\n",
    "f'(x_0) \\approx \\frac{f(x_0) - f(x_0 - \\Delta x)}{\\Delta x} + \\cal{O}\\left(\\left(\\Delta x\\right)^{n_{left}}\\right).\n",
    "$$\n",
    "* Central derivative:\n",
    "$$\n",
    "f'(x_0) \\approx \\frac{f(x_0 + \\Delta x) - f(x_0 - \\Delta x)}{2 \\Delta x} + \\cal{O}\\left(\\left(\\Delta x\\right)^{n_{central}}\\right).\n",
    "$$\n",
    "* Right derivative:\n",
    "$$\n",
    "f'(x_0) \\approx \\frac{f(x_0 + \\Delta x) - f(x_0)}{\\Delta x} + \\cal{O}\\left(\\left(\\Delta x \\right)^{n_{right}}\\right).\n",
    "$$\n",
    "\n",
    "1. Determine, using Taylor expansions to the appropriate order around $x_0$ the values of $n_{right}$, $n_{center}$, and $n_{left}$. \n",
    "2. According to the answer to the previous question, which formula should be used for the numerical estimation of $f'(x_0)$ using divided differences?.\n",
    "3. For the numerical estimation of $f'(x_0)$, with  $x_0 \\neq 0.0$, one should use $\\Delta x = x_0 h$, for some small $h$. Explain why.\n",
    "\n",
    "Our goal is to determine whay is the value of $h$ that is optimal, in the sense that the error of the numerical estimation of the derivatives by divided differences is as accurate as possible.\n",
    "\n",
    "4. Determine by numerical exploration the optimal value of $h$ of each of the three formulas given earlier for the functions $f(x) = e^x$  at $x_0 = 1.0$.\n",
    "5. Taking into consideration rounding and trunction errors, explain the behaviour observed of the error as a function of $h$. If possible provide an estimate of such errors for the values of $h$ considered.\n",
    "6. Provide numerical estimates of the optimal value of $h$ for $f(x) = \\log{x}$ at $x_0 = 10^{-30}$, at $x_0 = 1.0$, and at $x_0 = 10^{30}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0d37f",
   "metadata": {},
   "source": [
    "### Exercise 5: Zeros of a function\n",
    "\n",
    "1. Implement the bisection method. \n",
    "2. Implement the Newton-Raphson method.\n",
    "3. Implement the method of the secant.\n",
    "4. Vectorize the Newton-Raphson algorithm so that if an array of seeds is passed as an argument, the result is an array with the corresponding roots of the function.\n",
    "6. From an iid sample $\\left\\{U_m \\right\\}_{m=1}^M, Z_m \\sim U[0, 1]$, generate an iid sample $\\left\\{Z_m \\right\\}_{m=1}^M, Z_m \\sim N(0,1)$ standard normal random numbers using the method of the inverse:\n",
    "$$   \n",
    "\\begin{eqnarray}\n",
    "U_m & \\sim & U[0, 1] \\\\\n",
    "Z_m & = & norminv\\left(U_m\\right), \\quad m =1,\\ldots, M.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "The inverse of the cdf of the standard normal distribution ($norminv$) should be calculated using only the functions `norm.pdf` and `norm.cdf` from `scipy.stats` https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html, and the vectorized implementation of Newton-Raphson's method.\n",
    "7. Generate a sample of size $M=10000$ and illustrate that the random numbers generated follow a standard normal distribution:\n",
    "    1. Plot the empirical pdf (histogram) with $ nbins = 50$. Superimpose in the plot the scaled pdf.\n",
    "    2. In a separate figure, make a normal probability plot.  https://en.wikipedia.org/wiki/Normal_probability_plot\n",
    "    3. Generate a sample of the same size using the $numpy.random$ module. https://numpy.org/doc/stable/reference/random/index.html#random-quick-start. \n",
    "    4. Compare the two samples by making a quantile-quantile plot. https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af5dec",
   "metadata": {},
   "source": [
    "## Order of convergence.\n",
    "\n",
    "In many numerical estimation methods the goal is to estimate a quantity, say $x^*$, using an iterative procedure. \n",
    "\n",
    "In an iterative algorithm one builds sequence $x_0, x_1, \\ldots, x_n, \\ldots$, which is expected to converge in the limit of large $n$ to the solution\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} x_n = x^*.\n",
    "$$\n",
    "\n",
    "Assuming that the sequence has a limit (converges), the error of the approximation, given by the unknown quantity $ (x_n - x^*)$, can be approximated by \n",
    "$$\n",
    "e_n =  \\left|x_n - x_{n-1}\\right|.\n",
    "$$\n",
    "\n",
    "For large $n$, the error is expected to decrease as  \n",
    "$$ \n",
    "e_n \\sim C \\, (e_{n-1})^{\\alpha}, \\ \\ n \\rightarrow \\infty \n",
    "$$ \n",
    "where $alpha$ is the order of (local) convergence of the algorithm, and \n",
    "$$ \n",
    "C = \\lim_{n \\rightarrow \\infty} \\frac{e_n}{(e_{n-1})^{\\alpha}}.\n",
    "$$ \n",
    "\n",
    "\n",
    "For instance, if $\\alpha = 1$, the convergence is linear, for $\\alpha = 2$ quadratic, and so on.\n",
    "\n",
    "#### Example: The method of the bisection\n",
    "In the method of the bisection $ e_{n} = \\frac{1}{2} e_{n-1}.$\n",
    "Therefore, the method has *linear convergence*. Furthermore, it always converges (global convergence) if the function changes sign between the endpoints of the initial interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63460a7",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Order of convergence of the Newton-Raphson method.\n",
    "\n",
    "Consider the function $f(x)$ whose derivative $f'(x)$ is also known. Our goal is to find a vale $x = x^*$ such that $f(x^*) = 0.0$.\n",
    "\n",
    "Newton-Raphson's method for the estimation of the zero consists of the following iteration, starting at x_0 (initial seed, given): \n",
    "$$ x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}, n = 1, 2,...  $$ \n",
    "\n",
    "In this manner, one obtains the sequence $x_0, x_1, \\ldots, x_n, \\ldots$, which is expected to converge in the limit of large n\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} x_n = x^*.\n",
    "$$\n",
    "\n",
    "The algorithm is based on the following strategy: \n",
    "* Consider $x_{n-1}$, the estimate of $x^*$ at iteration $n-1$ of the algorithm.\n",
    "* Assuming that we are close to convergence, we can improve the approximation of $x^*$ by finding the zero of the tangent of $f(x)$ at $x = x_{n-1}$. The formula of the tangent is\n",
    "$$\n",
    "t(x) = f(x_{n-1}) + f'(x_{n-1}) \\left(x - x_{n-1}\\right).\n",
    "$$\n",
    "We define $x_n$ as the value of $x$ such that $t(x_n) = 0$\n",
    "$$\n",
    "f(x_{n-1}) + f'(x_{n-1}) \\left(x_n - x_{n-1}\\right) = 0 \\ \\ \\Longrightarrow x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}.  \n",
    "$$\n",
    "\n",
    "1. Does the algorithm guarantee finding a zero? (global convergence).\n",
    "2. Does the algorithm guarantee finding a zero that is suffiently close to the seed? (local convergence).\n",
    "2. Determine the order of (local) convergence (i.e., the value of $\\alpha$) of this algorith.\n",
    "3. Provide a graphical illustration of this order of convergence for the function $f(x) = e^{-x} - x$.\n",
    "4. Taking into account rounding error, what is the smallest value (order of magnitude) of the relative error $\\left(\\frac{e_n}{x_n} \\right)$ that we can expect to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586b82a",
   "metadata": {},
   "source": [
    "### Exercise 6: Quadratures\n",
    "1. Use broadcasting to compute using Simpson's rule the quadrature \n",
    "$$ \\int_a^b \\mathbf{f}(x) dx, \\quad \\quad \\mathbf{f}^{\\top}(x) = \\left(f_1(x), \\ldots, f_N(x)\\right)$$ \n",
    "in a vectorized manner.\n",
    "\n",
    "2. Consider the random vector $\\mathbf{X} \\sim \\text{pdf}(\\mathbf{x})$, with $X \\in \\mathbb{R}^D$. \n",
    "Code a function to compute the expected value of the function $g: \\mathbb{R}^D \\longrightarrow \\mathbb{R}$ is $$\\mathbb{E}\\left[ g\\left(\\mathbb{X}\\right) \\right] = \\int_{\\mathbf{x} \\in \\mathbf{R}^D} \\, d\\mathbf{x} \\, \\text{pdf}(\\mathbf{x}) \\, g(\\mathbf{x})\n",
    "$$ \n",
    "Use `quad` for $D=1$, `dblquad` for $D=2$, and `tplquad` for $D=3$ from the module `scipy.integrate`. \n",
    "    \n",
    "3. Derive the complexity of the algorithms assuming that the grid used is the same in all the dimensions.\n",
    "    1. Monitor the complexity of the algorithm as a function of the dimension $D$. \n",
    "    2. How should one implement the quadrature for $D \\ge 4$?\n",
    "     \n",
    "4. Consider the pdf of the $D$-dimensional multivariate Gaussian distribution\n",
    "$$\n",
    "\\text{norm.pdf}\\left(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) = \\frac{1}{\\sqrt{(2\\pi)^D |\\boldsymbol{\\Sigma}|}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right),\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\boldsymbol{\\mu} \\in \\mathbb{R}^D \\text{  is the mean vector}. \\\\\n",
    "& \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D} \\times \\mathbb{R}^{D}  \\text{  is the covariance matrix}.\n",
    "& \\left|\\boldsymbol{\\Sigma}\\right| \\text{   is the determinant of the covariance matrix} \\boldsymbol{\\Sigma}. \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "1 & = \\int_{\\mathbf{x} \\in \\mathbb{R}^D}  \\, d\\mathbf{x} \\, \\text{norm.pdf}\\left(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right), \\\\\n",
    "\\boldsymbol{\\mu} & = \\mathbb{E}\\left[\\mathbf{X}\\right] = \\int_{\\mathbf{x} \\in \\mathbb{R}^D}  \\, d\\mathbf{x} \\, \\text{norm.pdf}\\left(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) \\, \\mathbf{x}, \\\\\n",
    "\\boldsymbol{\\Sigma} & = \\text{var}\\left(\\mathbf{X}\\right) = \\mathbb{E}\\left[\\left(\\mathbf{X} - \\boldsymbol{\\mu} \\right) \\, \\left(\\mathbf{X} - \\boldsymbol{\\mu} \\right)^{\\top}\\right] = \\int_{\\mathbf{x} \\in \\mathbb{R}^D}  \\, d\\mathbf{x} \\, \\text{norm.pdf}\\left(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) \\, \\left(\\mathbf{x} - \\boldsymbol{\\mu} \\right) \\, \\left(\\mathbf{x} - \\boldsymbol{\\mu} \\right)^{\\top}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Illustrate these relations using a deterministic numerical quadrature algorithm for $D = 2$, $D=3$, and $D=4$. \n",
    "* Illustrate these relations using Montecarlo quadrature for $D = 2$, $D=3$, and $D=4$.\n",
    "* Comment the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68540595",
   "metadata": {},
   "source": [
    "### Exercise 7: Numerical, symbolic, and automatic differentiation\n",
    "\n",
    "1. Give a summary of differences between these three computational techniques to compute derivatives.\n",
    "2. Give examples of use for automatic differentiation in the following platforms \n",
    "    1. Tensorflow. https://www.tensorflow.org/guide/autodiff\n",
    "    2. Pytorch. https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "    3. Ezyme (Julia). https://enzyme.mit.edu/julia/stable\n",
    "\n",
    "**References**\n",
    "\n",
    "Atılım Günes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2017. \n",
    "Automatic differentiation in machine learning: a survey. \n",
    "J. Mach. Learn. Res. 18, 1 (January 2017), 5595–5637.\n",
    "https://www.jmlr.org/papers/volume18/17-468/17-468.pdf\n",
    "\n",
    "\n",
    "https://github.com/greyhatguy007/Mathematics-for-Machine-Learning-and-Data-Science-Specialization-Coursera/blob/main/C2/w1/C2_W1_Lab_1_differentiation_in_python.ipynb\n",
    "\n",
    "https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/readings/L06%20Automatic%20Differentiation.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa9e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
